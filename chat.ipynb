{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试集\n",
    "import json \n",
    "from data import TextDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "test_data = load_data('./src/dataset/test.json')[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.net import Net\n",
    "from src.model.embedding.token_embedding import Embedding\n",
    "from src.model.embedding.position import PositionalEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载模型\n",
    "gpt = Net()\n",
    "gpt.load_state_dict(torch.load('./trainer/gpt_100_.pth', map_location='cuda:0' if torch.cuda.is_available() else 'cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedd = Embedding(vocab_size=128000, dim=64)\n",
    "position_emb = PositionalEmbedding(max_len=3, dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'They are discussing a'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码器\n",
    "from tokenization.tokenizer import tokenizer\n",
    "max_length = 3\n",
    "tokenizer = tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7009, 527, 25394, 264]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(test_data[0]['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64])\n"
     ]
    }
   ],
   "source": [
    "# 词编码\n",
    "word_embedding = embedd(torch.tensor(tokenizer.encode(test_data[0]['input'])[:max_length]))\n",
    "# 位置编码\n",
    "position_embedding = position_emb(word_embedding)\n",
    "# 词向量\n",
    "context = word_embedding + position_embedding\n",
    "\n",
    "print(context.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word = gpt(context.to(device='cuda:0'if torch.cuda.is_available() else 'cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word = tokenizer.decode([next_word.argmax(-1).item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "def writer_output(text, delay=0.1):\n",
    "    \"\"\"\n",
    "    模拟打字机效果逐字输出文本\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        需要逐字输出的文本\n",
    "    delay : float, optional\n",
    "        每个字符输出的延迟时间，默认为 0.1 秒\n",
    "    \"\"\"\n",
    "    for char in text:\n",
    "        sys.stdout.write(char)  # 输出字符\n",
    "        sys.stdout.flush()      # 刷新输出缓冲区\n",
    "        time.sleep(delay)       # 延迟\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They are discussing agift"
     ]
    }
   ],
   "source": [
    "text = test_data[0]['input'] + next_word\n",
    "\n",
    "writer_output(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(input_text, max_length, index):\n",
    "    # 词编码\n",
    "    word_embedding = embedd(torch.tensor(tokenizer.encode(input_text)[index:index+max_length]))\n",
    "    # 位置编码\n",
    "    position_embedding = position_emb(word_embedding)\n",
    "    # 词向量\n",
    "    context = word_embedding + position_embedding\n",
    "\n",
    "    next_word = gpt(context.to(device='cuda:0'if torch.cuda.is_available() else 'cpu'))\n",
    "    # 解码\n",
    "    next_word = tokenizer.decode([next_word.argmax(-1).item()])\n",
    "\n",
    "    return next_word, input_text + ' ' + next_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_alltext(text, length):\n",
    "\n",
    "    tmp = ''\n",
    "    for index in range(50):\n",
    "        next_word, temp = generate_response(input_text=text, max_length=length, index=index)\n",
    "        text = temp\n",
    "        tmp += ' '+next_word\n",
    "\n",
    "    return tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 滑动窗口\n",
    "\n",
    "def slide_text_output(input_text, max_length):\n",
    "\n",
    "    # 逐步生成并显示字符\n",
    "    full_text = \"\"\n",
    "    for displayed_text in generate_alltext(text=input_text, length=max_length):\n",
    "        full_text += displayed_text  # 获取每次生成的文本\n",
    "        yield full_text  # 逐步返回给 Gradio 界面\n",
    "        time.sleep(0.01)  # 控制显示速度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"f:\\python\\miniconda3\\lib\\site-packages\\gradio\\queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"f:\\python\\miniconda3\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"f:\\python\\miniconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1935, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"f:\\python\\miniconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1532, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "  File \"f:\\python\\miniconda3\\lib\\site-packages\\gradio\\utils.py\", line 671, in async_iteration\n",
      "    return await iterator.__anext__()\n",
      "  File \"f:\\python\\miniconda3\\lib\\site-packages\\gradio\\utils.py\", line 664, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "  File \"f:\\python\\miniconda3\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"f:\\python\\miniconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"f:\\python\\miniconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"f:\\python\\miniconda3\\lib\\site-packages\\gradio\\utils.py\", line 647, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "  File \"f:\\python\\miniconda3\\lib\\site-packages\\gradio\\utils.py\", line 809, in gen_wrapper\n",
      "    response = next(iterator)\n",
      "  File \"C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_9824\\3520793653.py\", line 7, in slide_text_output\n",
      "    for displayed_text in generate_alltext(text=input_text, length=max_length):\n",
      "  File \"C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_9824\\2659926256.py\", line 5, in generate_alltext\n",
      "    next_word, temp = generate_response(input_text=text, max_length=length, index=index)\n",
      "  File \"C:\\Users\\1\\AppData\\Local\\Temp\\ipykernel_9824\\359785015.py\", line 7, in generate_response\n",
      "    context = word_embedding + position_embedding\n",
      "RuntimeError: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 1\n"
     ]
    }
   ],
   "source": [
    "# 前端界面\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.HTML(\"\"\"<h1 align=\"center\">Mini GPT-2</h1>\"\"\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3):\n",
    "            query = gr.Textbox(placeholder='输入内容:', lines=2, label='Content')\n",
    "            with gr.Row():\n",
    "                answer = gr.Textbox(placeholder='对话结果：', lines=2, label='Content')\n",
    "            with gr.Row():\n",
    "                submit = gr.Button('提交', variant='primary')\n",
    "                clear = gr.Button('清空', variant='secondary')\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            max_length = gr.Slider(0, 3, value=99, step=1.0, label=\"Maximum length\", interactive=True)\n",
    "            top_p = gr.Slider(0, 1, value=0.8, step=0.01, label=\"Top P\", interactive=True)\n",
    "            temperature = gr.Slider(0, 1, value=0.95, step=0.01, label=\"Temperature\", interactive=True)\n",
    "\n",
    "    submit.click(slide_text_output, inputs=[query, max_length], outputs=[answer], show_progress=True)\n",
    "    clear.click(lambda: \"\", None, answer)  \n",
    "    demo.queue().launch(share=False, inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
