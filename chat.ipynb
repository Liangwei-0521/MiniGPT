{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试集\n",
    "import json \n",
    "from data import TextDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "test_data = load_data('./src/dataset/test.json')[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.net import Net\n",
    "from src.model.embedding.token_embedding import Embedding\n",
    "from src.model.embedding.position import PositionalEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载模型\n",
    "gpt = Net()\n",
    "gpt.load_state_dict(torch.load('./trainer/gpt_100_.pth', map_location='cuda:0' if torch.cuda.is_available() else 'cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedd = Embedding(vocab_size=128000, dim=64)\n",
    "position_emb = PositionalEmbedding(max_len=3, dim=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'They are discussing a'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编码器\n",
    "from tokenization.tokenizer import tokenizer\n",
    "max_length = 3\n",
    "tokenizer = tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7009, 527, 25394, 264]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(test_data[0]['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64])\n"
     ]
    }
   ],
   "source": [
    "# 词编码\n",
    "word_embedding = embedd(torch.tensor(tokenizer.encode(test_data[0]['input'])[:max_length]))\n",
    "# 位置编码\n",
    "position_embedding = position_emb(word_embedding)\n",
    "# 词向量\n",
    "context = word_embedding + position_embedding\n",
    "\n",
    "print(context.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word = gpt(context.to(device='cuda:0'if torch.cuda.is_available() else 'cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_word = tokenizer.decode([next_word.argmax(-1).item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "def writer_output(text, delay=0.1):\n",
    "    \"\"\"\n",
    "    模拟打字机效果逐字输出文本\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        需要逐字输出的文本\n",
    "    delay : float, optional\n",
    "        每个字符输出的延迟时间，默认为 0.1 秒\n",
    "    \"\"\"\n",
    "    for char in text:\n",
    "        sys.stdout.write(char)  # 输出字符\n",
    "        sys.stdout.flush()      # 刷新输出缓冲区\n",
    "        time.sleep(delay)       # 延迟\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They are discussing agift"
     ]
    }
   ],
   "source": [
    "text = test_data[0]['input'] + next_word\n",
    "\n",
    "writer_output(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(input_text, max_length):\n",
    "    # 词编码\n",
    "    word_embedding = embedd(torch.tensor(tokenizer.encode(input_text)[:max_length]))\n",
    "    # 位置编码\n",
    "    position_embedding = position_emb(word_embedding)\n",
    "    # 词向量\n",
    "    context = word_embedding + position_embedding\n",
    "\n",
    "    next_word = gpt(context.to(device='cuda:0'if torch.cuda.is_available() else 'cpu'))\n",
    "    next_word = tokenizer.decode([next_word.argmax(-1).item()])\n",
    "\n",
    "    response = input_text + ' ' + next_word\n",
    "    # 模拟逐步输出\n",
    "    displayed_text = \"\"\n",
    "    for char in response:\n",
    "        displayed_text += char\n",
    "        time.sleep(0.05)  # 每个字符延迟 50ms\n",
    "        yield displayed_text  # 逐步更新输出\n",
    "\n",
    "    # return input_text + ' ' + next_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 前端界面\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.HTML(\"\"\"<h1 align=\"center\">Mini GPT-2</h1>\"\"\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3):\n",
    "            query = gr.Textbox(placeholder='输入内容:', lines=2, label='Content')\n",
    "            with gr.Row():\n",
    "                answer = gr.Textbox(placeholder='对话结果：', lines=2, label='Content')\n",
    "            with gr.Row():\n",
    "                submit = gr.Button('提交', variant='primary')\n",
    "                clear = gr.Button('清空', variant='secondary')\n",
    "\n",
    "        with gr.Column(scale=1):\n",
    "            max_length = gr.Slider(0, 3, value=99, step=1.0, label=\"Maximum length\", interactive=True)\n",
    "            top_p = gr.Slider(0, 1, value=0.8, step=0.01, label=\"Top P\", interactive=True)\n",
    "            temperature = gr.Slider(0, 1, value=0.95, step=0.01, label=\"Temperature\", interactive=True)\n",
    "\n",
    "    submit.click(generate_response, inputs=[query, max_length], outputs=[answer], show_progress=True)\n",
    "    clear.click(lambda: \"\", None, answer)  \n",
    "    demo.queue().launch(share=False, inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
